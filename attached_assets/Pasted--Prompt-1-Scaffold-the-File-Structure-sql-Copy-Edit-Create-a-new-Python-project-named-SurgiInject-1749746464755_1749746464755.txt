 Prompt 1 â€” Scaffold the File Structure
sql
Copy
Edit
Create a new Python project named `SurgiInject` with the following structure. Only generate empty files for now:

surgiinject/
â”œâ”€â”€ cli.py
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ injector.py
â”‚   â”œâ”€â”€ parser.py
â”‚   â”œâ”€â”€ prompty.py
â”‚   â””â”€â”€ diff.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral_client.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ fix_mobile_blank_bug.txt
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_injector.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

Use Python 3.10+, and make it ready to run in Replit. No code yet.
ðŸ”§ Prompt 2 â€” CLI Entry Point (cli.py)
markdown
Copy
Edit
Implement `cli.py` for a CLI tool with one command:

Command: `inject`
Arguments:
- `--file`: path to source file (required)
- `--prompt`: path to prompt template (required)
- `--apply`: optional flag, if present writes changes to file, else shows a diff

Logic:
1. Read target file
2. Read prompt template
3. Pass both to `engine/injector.py`
4. Print result (or save if `--apply` used)
ðŸ”§ Prompt 3 â€” Core Injector (injector.py)
rust
Copy
Edit
In `engine/injector.py`, implement a function `run_injection(source_code: str, prompt_template: str) -> str`.

Steps:
1. Format the prompt using prompty.py
2. Send to `models/mistral_client.py` (stub for now, just echo)
3. Return modified code

Use placeholder model response logic for now (e.g. insert a comment).
ðŸ”§ Prompt 4 â€” Prompt Builder (prompty.py)
yaml
Copy
Edit
In `engine/prompty.py`, implement `build_prompt(file_path: str, code: str, task: str) -> str`.

Format:
ðŸ“„ FILE: {file_path}
ðŸ§  TASK: {task}
ðŸ’¬ CONTEXT:
{code}

ðŸš€ INSTRUCTION:
{prompt from .txt file}

Return the final prompt string.
ðŸ”§ Prompt 5 â€” Dummy Mistral Model Client (mistral_client.py)
sql
Copy
Edit
In `models/mistral_client.py`, define a `run_model(prompt: str) -> str`.

Return a dummy modified version of the code. For now, simulate model response by appending:
"# [INJECTED COMMENT]"

Later this will connect to a real LLM endpoint.
ðŸ§  Final Instructions for Replit
After these 5 prompts, youâ€™ll have:

A working CLI (python cli.py inject ...)

Prompt templating

Injection mock flow

Ready to plug into real models (Groq, OpenAI, etc.)

